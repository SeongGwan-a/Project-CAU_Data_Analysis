{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "골목대장팀_파이썬코드2(크롤링).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongGwan-a/Project-CAU_Data_Analysis/blob/main/%EA%B3%A8%EB%AA%A9%EB%8C%80%EC%9E%A5%ED%8C%80_%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%BD%94%EB%93%9C2(%ED%81%AC%EB%A1%A4%EB%A7%81).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFYJqGe4_wth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba7c76e-f76b-4f87-e1e9-978dfe036d00"
      },
      "source": [
        "! pip install selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in c:\\users\\pjg27\\anaconda3\\lib\\site-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\pjg27\\anaconda3\\lib\\site-packages (from selenium) (1.25.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYMXSSIS_wtp"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup \n",
        "from pandas import DataFrame\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.common.exceptions import ElementNotInteractableException\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from selenium.webdriver import Chrome\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.keys import Keys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjtjsQa_wtt"
      },
      "source": [
        "# 사전에 운영체제에 맞는 크롬드라이버 설치 필요 \n",
        "# 자신의 드라이버 경로 지정하기\n",
        "# https://chromedriver.chromium.org/downloads\n",
        "driver = webdriver.Chrome('chromedriver')\n",
        "time.sleep(1)\n",
        "input(\"자동으로 실행되는 크롬을 전체화면으로 설정해주세요.\\n설정후 엔터를 입력하세요.\\n\")\n",
        "# 자동으로 실행되는 크롬 창을 전체화면으로 설정해주세요."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HHFvSnZ_wty"
      },
      "source": [
        "### 골목 식당 상권별 출현회차\n",
        "# -> 종료 회차 기준\n",
        "# 용산구 해방촌 20회\n",
        "# 성동구 성수동 뚝섬 25회\n",
        "# 강동구 성내동 38회\n",
        "# 서대문구 포방터시장 44회\n",
        "# 용산 청파동 하숙골목 49회\n",
        "# 회기동 벽화골목 54회\n",
        "# 둔촌동 88회\n",
        "# 정릉 아리랑시장 92회\n",
        "# 홍제동 문화촌 골목 104회\n",
        "# 공릉동 기찻길 골목 109회\n",
        "# 도봉구 창동 골목 133회\n",
        "# 중곡동 시장 앞 골목 138회\n",
        "\n",
        "times_viedo=[20,25,38,44,49,54,88,92,104,109,133,138]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGbaLHQh_wt3",
        "outputId": "399c8666-4a5c-4d30-f2f7-2e852d4be296"
      },
      "source": [
        "#회차 검색\n",
        "for t in range(len(times_viedo)):\n",
        "    times=times_viedo[t]\n",
        "    url = 'https://tv.naver.com/search?query=\"백종원의 골목식당\" '+str(times)+\"회&page=1\" \n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    \n",
        "    # 영상 개수\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    a=[k.get_text() for k in soup.select('div.tit_box')]\n",
        "    num_clip = list(map(int, re.findall(\"\\d+\",a[0])))[0]\n",
        "    \n",
        "    # 회차별 리뷰 크롤링\n",
        "    review = []\n",
        "    \n",
        "    for nc in range(1, num_clip+1):\n",
        "        driver.find_element_by_xpath('//*[@id=\"clip_list\"]/div[{}]/div/a'.format(nc)).click()\n",
        "        time.sleep(2)\n",
        "        \n",
        "        driver.find_element_by_xpath('//*[@id=\"playerArea\"]').click()\n",
        "\n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, 'lxml')\n",
        "        a=[k.get_text() for k in soup.select('div.u_cbox_paginate')]\n",
        "        b=[k.get_text() for k in soup.select('div.u_cbox_head')]\n",
        "        # 댓글 없는 겨우\n",
        "        if np.max(list(map(int, re.findall(\"\\d+\",b[0])))) == 0 :\n",
        "            driver.get(url)\n",
        "            time.sleep(1)\n",
        "            continue\n",
        "        # 댓글이 하나인 경우\n",
        "        elif np.max(list(map(int, re.findall(\"\\d+\",a[0])))) == 1 :\n",
        "            max=1\n",
        "        \n",
        "        else : \n",
        "          # 댓글 페이지의 최대 max\n",
        "          driver.find_element_by_xpath('//*[@class=\"u_cbox_next u_cbox_next_end\"]').click()\n",
        "          time.sleep(1)\n",
        "          html = driver.page_source\n",
        "          soup = BeautifulSoup(html, 'lxml')\n",
        "          a=[k.get_text() for k in soup.select('div.u_cbox_page_wrap')]\n",
        "          max = np.max(list(map(int, re.findall(\"\\d\",a[0]))))\n",
        "          driver.find_element_by_xpath('//*[@class=\"u_cbox_pre u_cbox_pre_end\"]').click()\n",
        "          time.sleep(1)\n",
        "\n",
        "        #max 만큼 실행\n",
        "        for i in range(0,max):\n",
        "            if i%5==0 and i//5>0:\n",
        "                driver.find_element_by_xpath('//*[@class=\"u_cbox_next\"]').click()\n",
        "                time.sleep(0.5)\n",
        "            else :\n",
        "                driver.find_element_by_xpath('//*[@class=\"u_cbox_page\"][{}]'.format(i%5 + 1)).click()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            html= driver.page_source\n",
        "            soup= BeautifulSoup(html, 'lxml')\n",
        "            a=[k.get_text() for k in soup.select('div.u_cbox_text_wrap')]\n",
        "            for i in range(len(a)):\n",
        "                review.append(a[i])\n",
        "\n",
        "        driver.get(url)\n",
        "        time.sleep(1)\n",
        "        \n",
        "    # 회차별 댓글 모음\n",
        "    pd.DataFrame(np.array(review).flatten()).to_csv(\"review_{}.csv\".format(times_viedo[t]), index=False, header = False)\n",
        "    print(str(times_viedo[t])+\"번째 회차 댓글 모음 csv 파일 저장 완료\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "25번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "38번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "44번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "49번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "54번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "88번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "92번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "104번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "109번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "133번째 회차 댓글 모음 csv 파일 저장 완료\n",
            "138번째 회차 댓글 모음 csv 파일 저장 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "953GZRGUGkoa",
        "outputId": "e560e661-360b-4d3e-f937-e629990c340e"
      },
      "source": [
        "# 상도동 회차 댓글 크롤링 (143번째 회차)\n",
        "times=143\n",
        "url = 'https://tv.naver.com/search?query=\"백종원의 골목식당\" '+str(times)+\"회&page=1\" \n",
        "driver.get(url)\n",
        "time.sleep(1)\n",
        "\n",
        "# 영상 개수\n",
        "html = driver.page_source\n",
        "soup = BeautifulSoup(html, 'lxml')\n",
        "a=[k.get_text() for k in soup.select('div.tit_box')]\n",
        "num_clip = list(map(int, re.findall(\"\\d+\",a[0])))[0]\n",
        "\n",
        "# 회차별 리뷰 크롤링\n",
        "review = []\n",
        "\n",
        "for nc in range(1, num_clip+1):\n",
        "    driver.find_element_by_xpath('//*[@id=\"clip_list\"]/div[{}]/div/a'.format(nc)).click()\n",
        "    time.sleep(2)\n",
        "\n",
        "    driver.find_element_by_xpath('//*[@id=\"playerArea\"]').click()\n",
        "\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    a=[k.get_text() for k in soup.select('div.u_cbox_paginate')]\n",
        "    b=[k.get_text() for k in soup.select('div.u_cbox_head')]\n",
        "    # 댓글 없는 겨우\n",
        "    if np.max(list(map(int, re.findall(\"\\d+\",b[0])))) == 0 :\n",
        "        driver.get(url)\n",
        "        time.sleep(1)\n",
        "        continue\n",
        "    # 댓글이 하나인 경우\n",
        "    elif np.max(list(map(int, re.findall(\"\\d+\",a[0])))) == 1 :\n",
        "        max=1\n",
        "\n",
        "    else : \n",
        "      # 댓글 페이지의 최대 max\n",
        "      driver.find_element_by_xpath('//*[@class=\"u_cbox_next u_cbox_next_end\"]').click()\n",
        "      time.sleep(1)\n",
        "      html = driver.page_source\n",
        "      soup = BeautifulSoup(html, 'lxml')\n",
        "      a=[k.get_text() for k in soup.select('div.u_cbox_page_wrap')]\n",
        "      max = np.max(list(map(int, re.findall(\"\\d\",a[0]))))\n",
        "      driver.find_element_by_xpath('//*[@class=\"u_cbox_pre u_cbox_pre_end\"]').click()\n",
        "      time.sleep(1)\n",
        "\n",
        "    #max 만큼 실행\n",
        "    for i in range(0,max):\n",
        "        if i%5==0 and i//5>0:\n",
        "            driver.find_element_by_xpath('//*[@class=\"u_cbox_next\"]').click()\n",
        "            time.sleep(0.5)\n",
        "        else :\n",
        "            driver.find_element_by_xpath('//*[@class=\"u_cbox_page\"][{}]'.format(i%5 + 1)).click()\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        html= driver.page_source\n",
        "        soup= BeautifulSoup(html, 'lxml')\n",
        "        a=[k.get_text() for k in soup.select('div.u_cbox_text_wrap')]\n",
        "        for i in range(len(a)):\n",
        "            review.append(a[i])\n",
        "\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "\n",
        "# 회차별 댓글 모음\n",
        "pd.DataFrame(np.array(review).flatten()).to_csv(\"review_{}.csv\".format(times), index=False, header = False)\n",
        "print(str(times)+\"번째 회차 댓글 모음 csv 파일 저장 완료\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "143번째 회차 댓글 모음 csv 파일 저장 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MWpxukRGkod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}